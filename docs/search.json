[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Local LLama in Electron JS\n\n\n\n\n\nI too want my own native app with local Llama models and so can you\n\n\n\n\n\n\nDec 7, 2023\n\n\nKeith Stevens\n\n\n\n\n\n\n  \n\n\n\n\nKT Villa Overview\n\n\n\n\n\nHow I ended up building a booking website with too much Generative AI\n\n\n\n\n\n\nNov 30, 2023\n\n\nKeith Stevens\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Open Source Musings & Snow Time",
    "section": "",
    "text": "Welcome! Here’s where I write thoughts on various open source Large Language Model projects and random things I’m doing. I’m a former Staff Software Engineer from Google Translate and until I make it back the states, I’m spending some time exploring the world of smaller teams and companies. I sometimes take on contractor work related to LLMs. During winter, I spend a lot of time skiing in Hakuba Japan (if you find yourself there, say hello and I can show you around)."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Open Source Musings & Snow Time",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\n\n\n  \n\n\n\n\nLocal LLama in Electron JS\n\n\n\n\n\nI too want my own native app with local Llama models and so can you\n\n\n\n\n\n\nDec 7, 2023\n\n\nKeith Stevens\n\n\n\n\n\n\n  \n\n\n\n\nKT Villa Overview\n\n\n\n\n\nHow I ended up building a booking website with too much Generative AI\n\n\n\n\n\n\nNov 30, 2023\n\n\nKeith Stevens\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#upcoming-reviews",
    "href": "index.html#upcoming-reviews",
    "title": "Open Source Musings & Snow Time",
    "section": "Upcoming Reviews",
    "text": "Upcoming Reviews\n\nMaking KT-Villa with FastChat, LLaVa & RedwoodJS:\n\nA basic overview\nGluing Stable Diffusion and Llava together\nPiping in Zephyr-7b\nWhy the rest of the project was still hard\n\nEvaluate some interesting projects:\n\nModelFusion\nllama-cpp-python\nxorbitsai/inference\nLilac\n\nTry out some more models:\n\nteknium/OpenHermes-2.5-Mistral-7B\nteknium/Mistral-Trismegistus-7B\nliuhaotian/llava-v1.5-7b"
  },
  {
    "objectID": "posts/231206-local-llama-electron/index.html",
    "href": "posts/231206-local-llama-electron/index.html",
    "title": "Local LLama in Electron JS",
    "section": "",
    "text": "Recently I had to make a trip to China with my wife. These days, that means I’ll be disconnected from the regular internets and totally isolated from the ChatGPTs of the world or even my Linux server hosted open source LLMs. I can get by without these things when it comes to writing emails or writing code, but I do need my LLMs to explore the world of bad dad jokes and get pet care advice. How am I to survive? local models with internet free apps to the rescue!\n\n\n\n\n\nRight now, probably the best and easiest to use app for all this is LM Studio. They let you install a local native app and then download quantized (e.g. compressed) open source models that you run totally locally if you have a sufficiently good computer. I have a Mac M2 with 20 something gigs of ram so I can run any 7B model without question. That lets me get all the dad jokes I need to be productive. But what if LM studio didn’t exist? What would it take to make it? Or what if I felt like I needed something like LM Studio but with a few twists and turns? How hard is it to do?\n\n\n\n\n\nIf you take a peek at what LM Studio is doing in the background, it’s pretty obvious it’s an Electron JS app. This has been a pretty standard way to write a cross platform native app using pure Node.JS. You write up your little server like background process and then a bunch of client side HTML + javascript and with a little bit of compilation you get your own LM Studio, Slack, or Discord app. So, what did LM Studio do to get their app? Let’s dive in and find out."
  },
  {
    "objectID": "posts/231206-local-llama-electron/index.html#why-local-models-why-local-apps",
    "href": "posts/231206-local-llama-electron/index.html#why-local-models-why-local-apps",
    "title": "Local LLama in Electron JS",
    "section": "",
    "text": "Recently I had to make a trip to China with my wife. These days, that means I’ll be disconnected from the regular internets and totally isolated from the ChatGPTs of the world or even my Linux server hosted open source LLMs. I can get by without these things when it comes to writing emails or writing code, but I do need my LLMs to explore the world of bad dad jokes and get pet care advice. How am I to survive? local models with internet free apps to the rescue!\n\n\n\n\n\nRight now, probably the best and easiest to use app for all this is LM Studio. They let you install a local native app and then download quantized (e.g. compressed) open source models that you run totally locally if you have a sufficiently good computer. I have a Mac M2 with 20 something gigs of ram so I can run any 7B model without question. That lets me get all the dad jokes I need to be productive. But what if LM studio didn’t exist? What would it take to make it? Or what if I felt like I needed something like LM Studio but with a few twists and turns? How hard is it to do?\n\n\n\n\n\nIf you take a peek at what LM Studio is doing in the background, it’s pretty obvious it’s an Electron JS app. This has been a pretty standard way to write a cross platform native app using pure Node.JS. You write up your little server like background process and then a bunch of client side HTML + javascript and with a little bit of compilation you get your own LM Studio, Slack, or Discord app. So, what did LM Studio do to get their app? Let’s dive in and find out."
  },
  {
    "objectID": "posts/231206-local-llama-electron/index.html#your-own-electron-js-app-with-local-models",
    "href": "posts/231206-local-llama-electron/index.html#your-own-electron-js-app-with-local-models",
    "title": "Local LLama in Electron JS",
    "section": "Your own Electron JS app with local models",
    "text": "Your own Electron JS app with local models\n\n\n\n\n\nOur end goal is a local app running a model 100% on our local machine and ideally packaged in a single ElectronJS app without any other services. There’s lots of other (easier) ways to crack this nut but let’s aim for this goal because we’re pretending we don’t like accessing apps through our web browsers. So what do you need? Ultimately you need a way to run the quantized LLMs with your bare model CPU or Apple silicon. You could write that yourself or you could leverage llama.cpp like a reasonable person.\nBut llama.cpp is in C++, which last I checked is not javascript, nor typescript. Thankfully @withcatai has solved this problem for us, mostly, by writing node-llama-cpp. This builds llama.cpp as some linkable libraries and adds Javascript bindings so any (almost) Node.JS app can call local models directly form javascript land.\nTo get this working, let’s solve two key requirements:\n\nWe must use ESM modules. Javascript is notorious for having many flavors and none of them work well together. node-llama-cpp chose ESM modules so that’s what we have to pick.\nWe like to be lazy so let’s do the client side in ReactJS. That will introduce some additional challenges.\n\nI’ve done all this already for you with an app I call local-llama-electron, a very creative name. If you want to read the code for yourself, take a minute and come back. Or just fork it and move along without reading below, but you might miss a funny image or two.\nLet’s look at the hardest parts now. Going forward, I’m going to assume you’ve created a vanilla Electron JS app using Electron Forge or you’re reading my repository.\nFirst, ElectronJS doesn’t yet fully support ESM moduels, a hard requirement for node-llama-cpp, but in their upcoming version 28 release they will be. That gets us pretty far. We just need to install the beta releaes and make a few changes to our Electron App after creating it.\nnpm install --upgrade electron@beta\nThe other small change you likely need to do is make sure all the Electron config files are written as ESM modules. This should look like\nexport default {\n  buildIdentifier: \"esm\",\n  defaultResolved: true,\n  packagerConfig: {},\n  rebuildConfig: {},\n  ...\n}\nIf we wanted to write all the client side in bare bones HTML, CSS, and Javascript, we’d be done. But people like me, we want ReactJS and that means we need a tool like WebPack or Vite to bundle client side code into something sensible. Normally Vite handles ESM really well but Electron’s Vite plugin does not. So I forked it to make a one line change that treats everything as ESM instead of some other option. You can find that here.\nYou can install that with something like\nnpm install --upgrade \\\n  \"https://github.com/fozziethebeat/electron-forge-plugin-vite-esm#plugin-vite-esm\" \\\n  --include=dev \\\n  --save\nAre we done yet? Assuming we’ve followed the Electron Forge documentation on setting up ReactJS and Vite? Nope, because Vite does so much work for us, it now complicates node-llama-cpp in one tiny way. It tries to bundle the entire package up for us but manages to leave out the C++ resource libraries.\nI bet there’s a better way to fix this but I edited my vite.main.config.ts file to include this stanza:\nexport default defineConfig({\n  build: {\n    rollupOptions: {\n      external: [\n        \"node-llama-cpp\",\n      ],\n    },\n  },\n  ...\n});\nNow we’ve got a fully functioning independent Local Llama Electron App. But let’s go further and test the limits of what we could build with some additional work."
  },
  {
    "objectID": "posts/231206-local-llama-electron/index.html#breaking-out-of-the-electron-box",
    "href": "posts/231206-local-llama-electron/index.html#breaking-out-of-the-electron-box",
    "title": "Local LLama in Electron JS",
    "section": "Breaking out of the Electron Box",
    "text": "Breaking out of the Electron Box\nSo far we set ourselves a goal and we hit it hard. We wanted a single app we can package and distribute that lets us run local models as a native app and that we got. But right now there’s a few limitations to what node-llama-cpp can do:\n\nIt doesn’t support Multimodal models like Llava even though llama.cpp does.\nIt doesn’t support streaming (again even though llama.cpp does).\nTo my knowledge, no one has built a llama.cpp for SDXL Turbo, the latest fast version of Stable Diffusion that you can run locally with a python setup.\n\nSo let’s expand these shennanigans with these working bits just to see if it feels fun and useful. Later we can figure out how to get everything back into the single Electron box.\nAt the end of the day, we’ll end up with something absurd like this:\n\n\n\n\nflowchart LR\n  Client[React JS]\n  Server[Main]\n  NodeLlama(node-llama-cpp)\n  LocalLlamaPython[local-llama-python]\n  LocalSDXLTurbo[local-sdxl-turbo]\n  SDXLTurbo(sdxl-turbo)\n  Llava(llava-1.5-7b)\n\n  subgraph ElectronJS\n    Client --&gt; Server\n    Server --&gt; NodeLlama\n  end\n\n  subgraph Python Server 1\n    Server --&gt; LocalLlamaPython\n    LocalLlamaPython --&gt; Llava\n  end\n\n  subgraph Python Server 2\n    Server --&gt; LocalSDXLTurbo\n    LocalSDXLTurbo --&gt; SDXLTurbo\n  end\n\n\n\n\n\n\nPutting the Llava in the multimodal\nAs stated, llama.cpp already supports running Multimodal modals like Llava 1.5 7B. This is pretty rad because it lets us take an image, run it through an embedding step and then feed that into a standard LLM to get some text description. We can even add arbitrary prompting related to the image. To fancy up our prototype, we can use llama-cpp-python, which is very much like node-llama-cpp but done for Python. Not only does this support Llava models, it also provides an OpenAI compatible server supporting the vision features.\nThat means, just go on over to llama-cpp-python, install it, download your favorite multimodal modal and turn it on! For me that looked like\npython -m llama_cpp.server \\\n  --model ~/.cache/lm-studio/models/mys/ggml_llava-v1.5-7b/ggml-model-q5_k.gguf \\\n  --model_alias llava-1.5 \\\n  --clip_model_path ~/.cache/lm-studio/models/mys/ggml_llava-v1.5-7b/mmproj-model-f16.gguf \\\n  --chat_format llava-1-5 \\\n  --n_gpu_layers 1\nNOTE: One big caveat. If you’re running on MacOS with an M2 chip, you might have an impossible time installing version 0.2.20. I added my solution to this issue, maybe it’ll help you too.\nWith that setup in a separate process, we just need to do our very standard app building and call the new fake OpenAI endpoint in our main process:\nasync function analyzeImage(event) {\n  // Get yo images.\n  const { filePaths } = await dialog.showOpenDialog({\n    filters: [{ name: \"Images\", extensions: [\"jpg\", \"jpeg\", \"png\", \"webp\"] }],\n    properties: [\"openFile\"],\n  });\n  // Tell the client side that we got the file and give it our local protocol\n  // that's handled properly for electron.\n  event.reply(\"image-analyze-selection\", `app://${filePaths[0]}`);\n  // Later, this should actually call a node-llama-cpp model.  For now we call\n  // llama-cpp-python through the OpenAI api.\n  const result = await mlmOpenai.chat.completions.create({\n    model: \"llava-1.5\",\n    messages: [\n      {\n        role: \"user\",\n        content: [\n          { type: \"text\", text: \"What’s in this image?\" },\n          {\n            type: \"image_url\",\n            image_url: `file://${filePaths[0]}`,\n          },\n        ],\n      },\n    ],\n    stream: true,\n  });\n  // Get each returned chunk and return it via the reply callback.  Ideally\n  // there should be a request ID so the client can validate each chunk.\n  for await (const chunk of result) {\n    const content = chunk.choices[0].delta.content;\n    if (content) {\n      event.reply(\"image-analyze-reply\", {\n        content,\n        done: false,\n      });\n    }\n  }\n  // Let the callback know that we're done.\n  event.reply(\"image-analyze-reply\", {\n    content: \"\",\n    done: true,\n  });\n}\nNow we can let a user click a button, select and image, and get some fancy text like below.\n\n\n\n\n\n\n\nNow let’s do it with images\nWe’re not satisfied with just generative text, nor with image to text models. No. No. No. We want the whole enchilada. We want text to image generation running all locally so we can get ourselves a full fledged all modality generative AI app.\nSo let’s drop the whole C++ requirement and write a tiny little OpenAI compatible API server that hosts SDXL Turbo. With a proper python setup this is pretty easy and we can again call that server fro, our Electron app with a REST API call.\nI did that for you, even tho it’s pretty easy. It’s over at local-sdxl-turbo. Download it, install, and run. Running is as simple as\npython -m server --device mps\nThen you too can add this tiny bit of Javascript to get generative images in your Electron App:\nasync function generateImage(event, prompt) {\n  // Later, this should actually call a node-llama-cpp model.  For now we call\n  // llama-cpp-python through the OpenAI api.\n  const result = await imageOpenai.images.generate({\n    prompt,\n    model: \"sdxl-turbo\",\n  });\n  return result.data[0].b64_json;\n}\nAnd with your image prompting skills, you too can try to replicate high tech marketing theories as artistic masterpieces."
  },
  {
    "objectID": "posts/231206-local-llama-electron/index.html#recap",
    "href": "posts/231206-local-llama-electron/index.html#recap",
    "title": "Local LLama in Electron JS",
    "section": "Recap",
    "text": "Recap\nMy little Local Llama Electron app is by no means meant to be a real usable product. It’s janky. It’s kinda ugly (although I do like the DaisyUI cupcake color palette). It’s also hard to setup and deploy.\nBut it is a demonstration of what’s possible these days with local models. With a bit more extra work you can have a fully packaged system. To get there you just need to:\n\nReplicate some of the work done by node-llama-cpp to include support for multi-modal modals.\nDo the whole llama.cpp thing but for SDXL-turbo. I’m sure someone has done it and I just don’t know. If so, then you just need the javascript bindings.\n\nAnd then you have a pretty fancy pants multi-modal LLM app for anyone to use.\nI might get around to doing those extra steps and documenting them, but no promises. It turns out even writing this blog post while in China is a massive pain."
  },
  {
    "objectID": "posts/kt-villa-overview/index.html",
    "href": "posts/kt-villa-overview/index.html",
    "title": "KT Villa Overview",
    "section": "",
    "text": "KT Villa is a nifty little booking website I hand built to manage trips to my winter lodge in Hakuba Japan. For entirely silly reasons I infused it with way more Generative AI than needed. Why? Well it all starts with little Meiji Chocolates in Japan where each tube comes with some delightful little sticker representing a country somewhere in the world. I’ve been collecting these over the years and thought it would be fun to do the same thing with bar soap, each purchase comes with a little trading card that has an AI generated image and an AI generated character profile. Like NFTs, people could “claim” them on a website but unlike NFTs they’re intentionally worthless. Seems pretty doable but it requires a soap company selling real products, which I aspire to have one day but currently do not (message me if you’re keen to do something silly like this plz).\n\n\n\n\n\nI do however have an empty winter lodge in Hakuba Japan, a prime place to enjoy the snow. And I have many friends who wish to stay there on the regular. So instead of generated images and character cards for soap purchases, I did the same thing for trips to my winter lodge. And bam, that’s how we get KT Villa. As a fun aside, the Villa part comes from a previous name for our house, Chill Villa, when it was on AirBnB and the KT part comes from nicknames for my wife and myself, Koala and Tree. Cute right?\n\nPretty much all the hard part of the site is just your regular old booking website challenges. Managing users, managing booking details, making sure two trips don’t overlap, making sure timezones are all done right. Making it look visually okay. It’s an entirely normal website except when you make a booking, you get a SDXL (Stable Diffusion XL from Stability AI) generated image. To boot, I setup SDXL to run a different themed LoRA adapter every two weeks so with a bit of prompt magic and adapter swapping, every trip will generate a fairly unique image.\n\nBut I wanted something even sillier. Since I’ve been contracting with a team focusing on themed chatbots, I wanted to take it to the extreme. For every generated imaged, I let users create a character profile card using a multi-modal LLM. That gives us a pretty unpredictable chatbot profile and when paired with a decent small chat trained LLM, users can try chatting with their booking image and see what silliness entails. With time I also plan to somehow cram in Retrieval Augmented Generation somewhere using LangChain.\n\nI honestly doubt any of this is really useful, but it felt like a fun way to try blending together real services and products with a little bit of AI flair, predominantly for the purpose of zaney entertainment. And with the state of things today, it was pretty easy to do even with entirely self hosted models. Let’s find out just how easy."
  },
  {
    "objectID": "posts/kt-villa-overview/index.html#the-origins-for-kt-villa",
    "href": "posts/kt-villa-overview/index.html#the-origins-for-kt-villa",
    "title": "KT Villa Overview",
    "section": "",
    "text": "KT Villa is a nifty little booking website I hand built to manage trips to my winter lodge in Hakuba Japan. For entirely silly reasons I infused it with way more Generative AI than needed. Why? Well it all starts with little Meiji Chocolates in Japan where each tube comes with some delightful little sticker representing a country somewhere in the world. I’ve been collecting these over the years and thought it would be fun to do the same thing with bar soap, each purchase comes with a little trading card that has an AI generated image and an AI generated character profile. Like NFTs, people could “claim” them on a website but unlike NFTs they’re intentionally worthless. Seems pretty doable but it requires a soap company selling real products, which I aspire to have one day but currently do not (message me if you’re keen to do something silly like this plz).\n\n\n\n\n\nI do however have an empty winter lodge in Hakuba Japan, a prime place to enjoy the snow. And I have many friends who wish to stay there on the regular. So instead of generated images and character cards for soap purchases, I did the same thing for trips to my winter lodge. And bam, that’s how we get KT Villa. As a fun aside, the Villa part comes from a previous name for our house, Chill Villa, when it was on AirBnB and the KT part comes from nicknames for my wife and myself, Koala and Tree. Cute right?\n\nPretty much all the hard part of the site is just your regular old booking website challenges. Managing users, managing booking details, making sure two trips don’t overlap, making sure timezones are all done right. Making it look visually okay. It’s an entirely normal website except when you make a booking, you get a SDXL (Stable Diffusion XL from Stability AI) generated image. To boot, I setup SDXL to run a different themed LoRA adapter every two weeks so with a bit of prompt magic and adapter swapping, every trip will generate a fairly unique image.\n\nBut I wanted something even sillier. Since I’ve been contracting with a team focusing on themed chatbots, I wanted to take it to the extreme. For every generated imaged, I let users create a character profile card using a multi-modal LLM. That gives us a pretty unpredictable chatbot profile and when paired with a decent small chat trained LLM, users can try chatting with their booking image and see what silliness entails. With time I also plan to somehow cram in Retrieval Augmented Generation somewhere using LangChain.\n\nI honestly doubt any of this is really useful, but it felt like a fun way to try blending together real services and products with a little bit of AI flair, predominantly for the purpose of zaney entertainment. And with the state of things today, it was pretty easy to do even with entirely self hosted models. Let’s find out just how easy."
  },
  {
    "objectID": "posts/kt-villa-overview/index.html#the-tech-stack",
    "href": "posts/kt-villa-overview/index.html#the-tech-stack",
    "title": "KT Villa Overview",
    "section": "The Tech Stack",
    "text": "The Tech Stack\n\n\n\n\nflowchart LR\n  React[React JS]\n  Fastify[Fastify Server]\n  Postgres[Postgres Database]\n  SurfaceChat[Surface Chat FastAPI]\n  LLaVa(LLaVa LLM)\n  SDXL(SDXL + LoRAs)\n  Zephyr(Zephyr 7B Beta)\n  FastChat[FastChat FastAPI]\n\n  subgraph RedwoodJS\n    React --&gt; Fastify\n    Fastify --&gt; Postgres\n  end\n\n  subgraph Custom FastAPI Python Server \n    Fastify --&gt; SurfaceChat\n    SurfaceChat --&gt; LLaVa\n    SurfaceChat --&gt; SDXL\n  end\n\n  subgraph Standard FastChat Python Server\n    Fastify --&gt; FastChat\n    FastChat --&gt; Zephyr\n  end\n\n\n\n\n\nWhen building this, I wanted to keep to a fairly standard tech stack that let me easily separate out potentially re-usable parts from all the booking-specific parts. I also wanted to leverage tools I was already pretty familiar with since this is mostly to simplify managing my winter lodge and only partially for fun. That led to a few key decisions, RedwoodJS for the primary client side experience, Postgres for all database stuff, FastAPI for any custom LLM servers, and ready to go server like FastChat for whatever could fit in it.\nThat’s how we got to this horrible monstrosity with three docker images and way too many containers running on my single NVIDIA RTX A6000 powered server. But with the absurd fleet of containers and some simple docker compose scripts, its actually pretty simple.\nIf you haven’t tried it yet, RedwoodJS is a wrapper around a ReactJS client side experience and a Fastify backend Node.JS server. It pairs well with a database like Postgres and does a bunch of GraphQL magic that I don’t like thinking about. I find it a lot easier to get started than something like Next.JS since Redwood doesn’t have any weird blending between client side and server side behavior and just templates out the annoying parts. If you like something straight forward, I very much recommend RedwoodJS. And pair it with Daisy UI for styling everything and you can pretty quickly get a nice looking hand crafted website.\nFor all the image related components, both generating images and turning images into character profiles, I hacked together a very lightweight FastAPI python server that I call Surface Chat. Its pretty much me glueing together various transformers API tutorials so that I can run a baseline SDXL model and swap in a bunch of different LoRA adapters on demand. Crammed in is LLaVA for multi-modal models mostly because I didn’t know of any open source framework that supported this with a standard format, but with OpenAI’s releases that will likely change soon.\nFinally, for the chat portion and other random places I might need pure a generative text model, I spun up FastChat with a single Zephyr 7B Beta model. FastChat has their own suite of tools for training but I primarily wanted their nice split framework so I can scale the heavy GPU portion easily and it’s support for various serving methods. And there’s probably newer options for this step but I haven’t investigated them yet. That is somewhere in my future."
  },
  {
    "objectID": "posts/kt-villa-overview/index.html#what-next",
    "href": "posts/kt-villa-overview/index.html#what-next",
    "title": "KT Villa Overview",
    "section": "What Next",
    "text": "What Next\nWhew that was a lot. This was really a whirlwind summary of what KT Villa is all about, why I made it, and a light sketch of what went into it. Up next I want to go into more detail about what’s happening inside SurfaceChat and generally what I like and don’t like about FastChat. Lastly I’ll go in depth through the RedwoodJS server and show where all the generative AI steps tie in and demonstrate how straightforward it can be with this kind of separation. Expect those in coming weeks.\nP.S. If you want to make a cute website like this, take @HamelHusain’s advice and look at Quarto. I used to use Jekyll and this is so so much better."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Keith Stevens",
    "section": "",
    "text": "With 20 years practicing Buddhism and 15 years working in Natural Language Processing, I firmly believe language technology works best when it helps people live economically better lives so they can spend more time building and enjoying the human relationships that give life meaning. More so, the best solutions come from teams that focus deeply on a poignant market problem and solve it with relentless focus. And that’s what I hope to keep doing.\nOver 9 years I worked with the google translate team as we did exactly that for machine translation. We got to live through the paradigm shift from old school statistical NLP to large transformer models, being one of the first teams in Google to productionize transformer models. With my direct teams we built systems that discovered, curated, and housed the vast pools of data needed to train these models. And with a small team I personally led we build out user facing features that let users share data and insights with us.\nSince leaving Google I spent the last two years seeing the startup world up close. I tried replicating some of my Google work as an open source project. Partnered with a co-founder to try making an emotional intelligence oriented AI startup. I’ve since been working with several startups to build out their LLM infused products. Until mid 2024, I plan to continue doing this will splitting my time between Tokyo and Hakuba Japan. In my spare time I’ll also be skiing and writing mini reports on various LLM adjacent open source projects that catch my interest.\nAfter the middle of 2024, I expect to relocate from Japan to either the SF Bay Area or Los Angeles while my wife studies for her MBA. While in the states, I hope to find a solid team to join more permanently to make sure LLMs are useful. If that’s you, let me know."
  }
]